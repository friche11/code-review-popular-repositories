from datetime import datetime
import http.client
import json
import os
from dotenv import load_dotenv

# Carregar vari√°veis do .env
load_dotenv()

GITHUB_API_URL = "api.github.com"
TOKEN = os.getenv("GITHUB_TOKEN")

def load_query(file_name):
    """Carrega a query GraphQL a partir de um arquivo .gql"""
    path = os.path.join(os.path.dirname(__file__), file_name)
    if not os.path.exists(path):
        raise FileNotFoundError(f"Arquivo '{file_name}' n√£o encontrado em: {path}")
    with open(path, "r") as file:
        return file.read()

def fetch_github_data(limit=200):
    """Busca os reposit√≥rios populares com base nos crit√©rios definidos"""
    if not TOKEN:
        raise ValueError("‚ùå GITHUB_TOKEN n√£o est√° definido.")

    conn = http.client.HTTPSConnection(GITHUB_API_URL)

    headers = {
        "Authorization": f"Bearer {TOKEN}",
        "Content-Type": "application/json",
        "User-Agent": "Python-Request"
    }

    QUERY_REPOS = load_query("query_repos.gql")

    repos = []
    after_cursor = None
    page = 1

    print(f"\nüöÄ Iniciando busca dos {limit} reposit√≥rios mais populares...")

    while len(repos) < limit:
        print(f"\nüìÑ P√°gina {page} | Cursor atual: {after_cursor}")
        variables = {"after": after_cursor}
        request_body = json.dumps({"query": QUERY_REPOS, "variables": variables})

        conn.request("POST", "/graphql", body=request_body, headers=headers)
        response = conn.getresponse()
        data = json.loads(response.read().decode("utf-8"))

        if "errors" in data:
            print(f"‚ùå Erro na requisi√ß√£o: {data['errors']}")
            break

        search = data.get("data", {}).get("search", {})
        new_repos = [edge["node"] for edge in search.get("edges", [])]
        print(f"üîé {len(new_repos)} reposit√≥rios encontrados nesta p√°gina")

        for repo in new_repos:
            repos.append(repo)

            if len(repos) % 20 == 0:
                print(f"üì¶ {len(repos)} reposit√≥rios acumulados at√© agora")

            if len(repos) >= limit:
                break

        if not search.get("pageInfo", {}).get("hasNextPage"):
            print("‚úÖ Fim da pagina√ß√£o: n√£o h√° mais p√°ginas.")
            break

        after_cursor = search.get("pageInfo", {}).get("endCursor")
        page += 1

    conn.close()
    print(f"\nüèÅ Busca finalizada. Total de reposit√≥rios coletados: {len(repos)}")
    return repos[:limit]


def fetch_prs_for_repo(conn, headers, query, repo_name):
    """Busca PRs com pelo menos 1 revis√£o e mais de 1h de vida"""
    print(f"  üîç Come√ßando busca de PRs para {repo_name}...")  # NOVO LOG
    owner, name = repo_name.split("/")
    after_cursor = None
    valid_prs = []
    page = 1

    while True:
        variables = {
            "owner": owner,
            "name": name,
            "after": after_cursor
        }
        body = json.dumps({"query": query, "variables": variables})
        conn.request("POST", "/graphql", body=body, headers=headers)
        response = conn.getresponse()
        data = json.loads(response.read().decode("utf-8"))

        if "errors" in data:
            print(f"‚ö†Ô∏è Erro ao buscar PRs para {repo_name}: {data['errors']}")
            break

        pr_nodes = data.get("data", {}).get("repository", {}).get("pullRequests", {}).get("nodes", [])
        for pr in pr_nodes:
            review_count = len(pr.get("reviews", {}).get("nodes", []))
            created = pr.get("createdAt")
            closed = pr.get("closedAt") or pr.get("mergedAt")
            if review_count >= 1 and created and closed:
                created_dt = datetime.strptime(created, "%Y-%m-%dT%H:%M:%SZ")
                closed_dt = datetime.strptime(closed, "%Y-%m-%dT%H:%M:%SZ")
                if (closed_dt - created_dt).total_seconds() >= 3600:
                    valid_prs.append(pr)

        page_info = data.get("data", {}).get("repository", {}).get("pullRequests", {}).get("pageInfo", {})
        if not page_info.get("hasNextPage"):
            break

        after_cursor = page_info.get("endCursor")
        page += 1

    print(f"üîÅ {repo_name}: {len(valid_prs)} PRs v√°lidos encontrados")
    return valid_prs
